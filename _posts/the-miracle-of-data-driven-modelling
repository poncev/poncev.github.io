---
layout: post
title: "The Miracle of Data-Driven Modelling"
author: "Felipe Ponce-Vanegas"
categories: general
---

# The Mircale of Data-Driven Modelling

A typical problem in machine learning is to fit a collection of labelled data points
$(\bold{x}^1, y^1), \ldots, (\bold{x}^p, y^p)$ using some model function
$$y = f(\bold{x}).$$
Each point $\bold{x} = (x_1, \ldots, x_N)$ is a vector in $\R^N$, where $N$ could easily be about millions.
For example, a black and white picture with a regular camera may has $1920\times 1080$ pixels,
so we can think of each photo as a vector $\bold{x}\in\R^N$, where $N = 1920\times 1080 = 2\,073\,600$
and the value of each variable $x_i$ is in a gray scale $0 \le x_i \le 1$, where
0 is white and 1 is black.

Sometimes people think that many numbers equals many data because
they lack clarity about their objectives and
fail to understand what is a single instance.
If the objective is to distinguish cats and dogs in pictures,
hundreds of photos with a cheap camera is much better than
two photos with a camera with thousands of pixels.
In fact, too many variables with few instances jeopardize the effectivness of the algorithms.

Since every vector $x$ is contained in the unit cube (or rather, hyper-cube) $[0,1]^N$,
one approach we can take to solve the problem of deciding whether there is cat in a photo
is to divide $[0,1]^N$ into cubes of side-length $l$.
The size of $l$ depends on what we will consider as different photos,
so we can measure the distance between two points as
$$\lVert x - y\rVert^2 := \sum_{i=1}^N \,\lvert x_i - y_i\rvert^2$$
and we can set a tolerance $\lVert x - y\rVert > \delta$ will be considered different.

<img src="../assets/img/the-miracle-of-data-driven-modelling/hypercube.png"
    alt="hypercube"
    display=block
    align=left
    width=30%
    height=auto />

If we want to divide $[0,1]^N$ into cubes, how small should they be?
The distance between a point in a cube of side-length $l$ and the center of the cube is at most
$$\lVert x - y\rVert \le \sqrt{N}(l/2)$$
and this bound cannot be improved.
To put it in context, suppose that each variable in $x$ is a measure in millimeters,
so if $N = 10^6$, then there are pairs of points in a cube of volume 1 $\textrm{mm}^N$ at 1 m of distance!
This is a manifestation of the curse of dimensionality; see also <a href="https://en.wikipedia.org/wiki/Concentration_of_measure" target="_blank"> Concentration of Measure </a>.

The length of our smaller cubes should be $l \approx \delta / \sqrt{N}$,
which is very small if $N$ is large.
Now, the number of cubes we need to tile $[0,1]^N$ is
$$M = l^N \approx (N/\sqrt{\delta})^{N/2}.$$
If $N = 10^6$, then $M$ is huge, larger than atoms in the earth, and it scales
terrible fast. How can we possible learn from data when we need so much?

The answer is that a typical picture is not a bunch of random pixels, but
they have some structure. So, the big challenge in Data Science is to discover
the structure of data.

## References

1. Mallat, S. (2016). Understanding deep convolutional networks, *Phil. Trans. R. Soc. A.* **374**: 20150203. [link][1]
2. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*, 2nd ed. Springer.

[1]: https://doi.org/10.1098/rsta.2015.020